# Document Q&A LLM Application with Docker & Hugging Face

A step-by-step guide to building and deploying a **Document Question-Answering Chatbot** using **Gradio**, **LlamaIndex**, **LlamaParse**, **Mixedbread AI**, **Groq**, **Docker**, and **Hugging Face Spaces**.

---

## ðŸ“Œ Project Overview
This project demonstrates how to:
- Build an **RAG (Retrieval-Augmented Generation)** based chatbot that allows users to upload documents and query them in real time.
- Integrate multiple AI services for parsing, embedding, and inference.
- Package and deploy the application using **Docker** to **Hugging Face Spaces**.

### **Tools Used**
- **Gradio** â€“ UI for document upload & chat
- **LlamaCloud / LlamaParse** â€“ Parse multiple document formats
- **Mixedbread AI** â€“ Generate document & query embeddings
- **Groq** â€“ Access high-speed LLM responses (`llama-3.1-70b-versatile`)
- **LlamaIndex** â€“ Orchestrate RAG pipeline
- **Docker** â€“ Containerization
- **Hugging Face Spaces** â€“ Cloud deployment

---

## ðŸ“‚ Project Structure
â”œâ”€â”€ app.py # Main application
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ Dockerfile # Container setup
â”œâ”€â”€ .env # API keys (Not committed to Git)
â””â”€â”€ .gitignore # Ignore sensitive files


---

## ðŸ”‘ Prerequisites
- **Python 3.9+**
- **Docker**
- API Keys:
  - LlamaCloud
  - Mixedbread AI
  - Groq Cloud

